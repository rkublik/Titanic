---
title: "Predicting Passenger Survival"
author: "Richard Kublik"
date: "November 29, 2017"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: 
      collapsed: false
      smooth_scroll: true
---

<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
      ga('create', 'UA-75601650-1', 'auto');
      ga('send', 'pageview');
      
</script>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

# Introduction

As an introductory data science project, I have chosen to explore the data provided by the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition hosted by Kaggle. The competition is to build the best model that can predict whether a given passenger survived the sinking of the Titanic. As a first step, I performed [introductory data analysis](http://http://portfolio.richard.crkublik.com/Titanic/output/exploratory.html) to learn more about the passengers on board. In this second part, I will compare different machine learning algorithms and submit my solution to Kaggle.

#Data Munging

We begin by loading the required packages, and performing the data munging steps described in Part 1. For the current prediction task, we are given a training dataset, and a testing dataset. Both datasets have missing data, and we will combine them prior to performing the data munging steps detailed in part 1.

```{r include=FALSE, results="hide"}
library(plyr)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(ranger)
library(e1071)
library(caTools)
library(pROC)
library(randomForest)
library(glmnet)
library(knitr)
library(kernlab)
library(party)
library(ggmosaic)
library(ggbiplot)

# Set seed for reproducible results
set.seed(234233343)
```


```{r}
# load the training dataset
train_data <- read_csv(file.path('..','data','train.csv'))
test_data <- read_csv(file.path('..','data','test.csv'))

# Preprocessing, data cleanup the same as for exploratory analysis
# initial pre-processing:
train_data <- train_data %>% 
  mutate(Survived = factor(Survived, levels = c(1, 0), labels = c("yes", "no")))

titanic_data <- train_data %>% 
  bind_rows(test_data) %>% 
  mutate(Pclass = factor(Pclass), 
         Sex = factor(Sex),
         Embarked = factor(Embarked),
         Title = factor(str_extract(Name, "[a-zA-z]+\\.")),
         FamilyName = str_extract(Name, "[a-zA-z]*"))
# Convert variable names to lowercase
names(titanic_data) <- tolower(names(titanic_data))

# Fill in missing values:

# CABIN
# find shared tickets:
ticket_deck <-  titanic_data %>% 
  filter(!is.na(cabin)) %>% 
  distinct(ticket, .keep_all = TRUE) %>% 
  select(ticket, cabin) %>% 
  mutate(deck = substr(cabin, 0, 1)) %>% 
  select(-cabin)
  

# fill in missing deck from same ticket
titanic_data <- titanic_data %>% 
  merge(ticket_deck, by = "ticket", all.x = TRUE) %>% 
  mutate(tdeck = substr(cabin, 0, 1),
         deck = ifelse(!is.na(tdeck), tdeck, deck),
         deck = ifelse(is.na(deck), "U", deck)) %>% 
  select(-tdeck) 



# FARE
missing_fare <- titanic_data %>% 
  filter(is.na(fare))
mean_fare <- titanic_data %>% 
  filter(pclass == missing_fare$pclass,
         embarked == missing_fare$embarked,
         sex == missing_fare$sex) %>% 
  summarize(mean_fare = mean(fare, na.rm = TRUE))
print(mean_fare)

titanic_data$fare[which(titanic_data$passengerid == missing_fare$passengerid)] <- mean_fare$mean_fare[1]

# EMBARKED
titanic_data$embarked[which(is.na(titanic_data$embarked))] <- "S"

# AGE
tb <- cbind(titanic_data$age, titanic_data$title)

# get the mean ages for each title
(age_dist <- titanic_data %>% 
  group_by(title) %>% 
  summarize(n = n(),
            n_missing = sum(is.na(age)),
            perc_missing = 100*n_missing/n,
            mean_age = mean(age, na.rm = TRUE),
            sd_age = sd(age, na.rm = TRUE),
            min_age = min(age, na.rm = TRUE),
            max_age = max(age, na.rm = TRUE)) %>% 
  filter(n_missing > 0))

titanic_data <- titanic_data %>% 
  mutate(age_est = 0)


bnorm <- function(nsamp, mean, std, lbound, ubound, rounding){
  samp <- round(rnorm(nsamp, mean, std), rounding)
  samp[samp < lbound] <- lbound
  samp[samp > ubound] <- ubound
  samp
}

for (key in c("Dr.", "Master.", "Miss.", "Mr.", "Mrs.")) {
  idx_na <- which(titanic_data$title == key & is.na(titanic_data$age))
  age_idx <- which(age_dist$title == key)
  titanic_data$age[idx_na] <- bnorm(length(idx_na), 
                                    age_dist$mean_age[age_idx], 
                                    age_dist$sd_age[age_idx],
                                    age_dist$min_age[age_idx],
                                    age_dist$max_age[age_idx],
                                    1)
  titanic_data$age_est[idx_na] <- 1
}
# impute single missing Ms. value to be the mean:
idx_na <- which(titanic_data$title == "Ms." & is.na(titanic_data$age))
age_idx <- which(age_dist$title == "Ms.")
titanic_data$age[idx_na] <- age_dist$mean_age[age_idx]
titanic_data$age_est[idx_na] <- 1


```
## Additional Feature Engineering
### Passenger Titles
We have already seen that there are many different titles assigned to the passengers:
```{r echo = FALSE}
titanic_data %>% 
  group_by(title, sex) %>% 
  summarize(Num_Passengers = n()) %>% 
  kable()
```

While these titles were informative, they add a level of complexity that may have a detremental effect on the machine learning models that we will be looking at. We will reduce the number of titles to 4: Mr., Mrs., Master., and Miss. These titles capture both gender and age information.

```{r }
titanic_data <- titanic_data %>% 
  mutate(title = as.character(title),
         title = ifelse((title == "Dr.") & sex == "female", "Mrs.", title),
         title = ifelse(title == "Mlle.", "Miss.", title),
         title = ifelse((title != "Miss.") & sex == "female", "Mrs.", title),
         title = ifelse((title != "Master.") & sex == "male", "Mr.", title),
         title = factor(title))
```

### Age Categories
While the passenger title can serve as a proxy for both gender and age (eg. Master refers to males under the age of 15), it is not very specific (eg. Miss refers to an unmarried woman of any age). We will create a new categorical variable for age with categories: child, teen, adult, senior.To get a sense of where the category splits should be, we look at the histogram of ages:
```{r}
titanic_data %>% 
  ggplot(aes(x = age, fill = factor(age_est, levels = c(1, 0)))) + 
  geom_histogram(binwidth = 1) +
  geom_rug() +
  ggtitle('Age distribution of passengers') +
  scale_fill_discrete(name = "Age",
                      breaks = c("0", "1"),
                      labels = c("Reported", "Estimated")) +
  labs(x = "Age", 
       y = "Number of Passengers")
```
We will set the range of ages: child: <= 10, teen 11-19, adult 20-59, senior >= 60.

```{r}
titanic_data <- titanic_data %>%  
    mutate(ageclass = ifelse(age < 10, 'child',
                           ifelse(age < 20, 'teen',
                                  ifelse(age < 60, 'adult', 'senior')
                                  )
                           )
         ) %>% 
  mutate(ageclass = as.factor(ageclass))
```

### Family Size
There are 2 variables that get at how large families are: *parch*, and *sibsp*. We create a *familysize* variable to combine these values.
```{r}
titanic_data <- titanic_data %>% 
  mutate(familysize = 1 + parch + sibsp)
```

### Remove unhelpful data
We remove variables that cannot be used for prediction
```{r}
titanic_data <- titanic_data %>% 
  select(-ticket, -name, -cabin, -familyname, -age_est)

```

## Data Exploration
In this section we look at the relationships between each variable and passenger survival. We begin by separating the data again into the training and testing datasets that were provided.
```{r}
# separate provided training/testing datasets
test_idx <- which(is.na(titanic_data$survived))
train_data <- titanic_data[-test_idx,]
test_data <-  titanic_data[test_idx,]
```

```{r echo = FALSE}
mosaic_plot <- function(data, varx, vary){
  stats <- data %>%
    count_(varx)
    
  total_rows <- dim(data)[1]
  data$weight = 0

  for (s in 1:dim(stats)[1]) {
    data$weight[which(data[[varx]] == stats[[1]][[s]])] = stats$n[[s]]/total_rows
  }
  
  data %>% 
    ggplot() +
    geom_mosaic(aes(weight = weight, x = product(get(vary), get(varx)), fill = get(vary))) +
    labs(x = varx,
         y = "Percent",
         title = sprintf("%s vs %s", vary, varx)) + 
    scale_fill_discrete(name = vary)
}
```
<div class="row">
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    mosaic_plot(train_data,"pclass","survived") 
```
  </div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(train_data,"sex","survived")  
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(train_data,"sibsp","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(train_data,"parch","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(train_data,"embarked","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(train_data,"title","survived")
```
</div>
<div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(train_data,"deck","survived")
```
</div>
<div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(train_data,"familysize","survived")
```
</div>
<div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(train_data,"ageclass","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo = FALSE}
train_data %>% 
    ggplot(aes_string(x = "survived", y = "age")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Survial",
       y = "Age",
       title = "Age distribution by survival")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
train_data %>% 
    ggplot(aes_string(x = "survived", y = "fare")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Survial",
       y = "Fare",
       title = "Fare distribution by survival")
```
</div>
</div>

From these plots we see some trends that we would expect:

* First class passengers were most likely to survive, and third class passengers least likely to survive
* Women were more likely to survive than men
* Survival rates by age category show that the younger you are, the more likely to survive
* Passengers with title Mr. were much less likely to survive than other passengers


And one that we wouldn't:

* Family size has an effect on survival, with survival rates increasing upto family size of 4, then decreasing dramatically

## PCA:

In many cases, Principal Component Analysis (PCA) can provide some insight into the data. By plotting the data in PCA space we can potentially see clustering in the data, along with information about co-linearity of the variables. Plotting the PCA of the input variables, we obtain:
```{r}
train_pca <- train_data %>% 
  select(-survived, -passengerid, -deck) %>%
  # convert factors to numeric values
  mutate(#survived = as.numeric(survived),
         pclass = as.numeric(pclass),
         sex = as.numeric(sex),
         age = as.numeric(age),
         sibsp = as.numeric(sibsp),
         parch = as.numeric(parch),
         fare = as.numeric(fare),
         embarked = as.numeric(embarked), 
         title = as.numeric(title),
         ageclass = as.numeric(ageclass),
         familysize = as.numeric(familysize)) %>% 
  # Calculate PCA
  prcomp(center = TRUE, scale = TRUE) 
  
ggbiplot(train_pca, group = train_data$survived) 
```

From this plot, we see that age, title, and ageclass are all essentially co-linear, as are embarked and pclass. We also see that the points depicting surviving and perishing passengers are not well separated. As a result, we don't expect to obtain a model with extremely high accuracy.

# Predictive Models
Using the insights we have gained in the previous section, well will train a number of predictive models to determine the best option for this case. We will use the caret package as it provides a nice wrapper for the different algorithms and takes care of many of the parameter optimization tasks. We begin by creating indicator variables for each level of each categorical variable, and scale the continuous variables

```{r}
center_scale <- function(m){
  mean = mean(m)
  std = sd(m)
  (m - mean)/std
}

transform_variables <- function(data){
  tmp <- data %>% mutate(age = center_scale(age),
                         fare = center_scale(fare),
                         sibsp = center_scale(sibsp),
                         parch = center_scale(parch),
                         familysize = center_scale(familysize))
  dummy <- dummyVars(~ ., data == tmp, sep = "_", drop2nd = TRUE)
  tdata <- data.frame(predict(dummy, newdata = tmp)) %>%   
    mutate(survived = as.factor(survivedyes)) %>%
    select(-survivedyes, -survivedno)
}

training_data <- transform_variables(train_data)
```

** center, scale, and make dummy variables to be used everywhere **


Finally, we divide the provided training data into training and validation sub-sets.
```{r}
train_idx <- createDataPartition(training_data$survived, p = 0.7, list = FALSE)
training <- list("transformed" = training_data[train_idx,],
                 "original" = train_data[train_idx,])
validation <- list("transformed" = training_data[-train_idx,],
                   "original" = train_data[-train_idx,])
```

*** need to make sure all levels of each variable are in training dataset ***
```{r}
unique(training$original$deck)
unique(validation$original$deck)
titanic_data %>% filter(deck == "T")

trow <- validation$original %>% filter(deck == "T")
training$original <- training$original %>% rbind(trow)
validation$original <- validation$original %>% filter(deck != T)

trow <- validation$transformed %>% filter(deckT == 1)
training$transformed <- training$transformed %>% rbind(trow)
validation$transformed <- validation$transformed %>% filter(deckT == 0)
```

## Logistic Regression
The first model we will consider is one of the simplest classification models: logistic regression.
We create a function that will allow us to easily evaluate the models we create:
```{r message = FALSE, warning = FALSE}
# define a common training control
train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)


# create a function to run the logistic regression for us.
evaluate_model <- function(train_data,test_data, interactions = FALSE){
  
  #train the model
  if (interactions) {
  model <-  train(survived ~ .*.,
                  data = train_data,
                  method = "glm",
                  metric = "ROC", 
                  trControl = train_control,
                  family = binomial(link = "logit"))
  } else {
    
  model <-  train(survived ~ .,
                  data = train_data,
                  method = "glm",
                  metric = "ROC", 
                  trControl = train_control,
                  family = binomial(link = "logit"))
  }
  
  pred <-  predict(model, test_data, type = "prob")
  analysis <- roc(response = test_data$survived, predictor = pred$yes)
  error <- cbind(analysis$thresholds, analysis$sensitivities + analysis$specificities)
  thresh <- subset(error, error[,2] == max(error[,2]))[,1]
  

  # determine confusion matrix for testing data
  # Turn probabilities into classes, and look at frequencies:
  #Because the logistic regression model provides probabilities of a passenger surviving or perishing, 
  #we need to determine the probability threshold that will be used to classify passengers.

  pred$survived <- ifelse(pred[['yes']] > thresh, 1, 0)
  pred$survived <- factor(pred$survived, levels = c(1, 0), labels = c("yes", "no"))
  conf <-  confusionMatrix(pred$survived, test_data$survived)

  
  perf <- c(conf$overall["Accuracy"],
            conf$byClass["Sensitivity"],
            conf$byClass["Specificity"])
  
  print(conf$table)
  print(perf)
  
  list('model' = model,
       'prediction' = pred,
       'analysis' = analysis,
       'threshold' = thresh,
       'conf' = conf,
       'perf' = perf
       )
}

plot_ROC_curve <- function(analysis, thresh){
    #Plot ROC Curve
  plot(1 - analysis$specificities,analysis$sensitivities,type = "l",
       ylab = "Sensitiviy",
       xlab = "1-Specificity",
       col = "black",
       lwd = 2,
       main = "ROC Curve for Simulated Data")
  abline(a = 0,b = 1)
  abline(v = thresh) #add optimal t to ROC curve
}

```

### Model based on intuition
We begin with a model that uses the variables we previously determined to be important.

```{r}
logistic_intuition <- evaluate_model(training$original %>% select(survived, pclass, sex, ageclass, familysize), 
                                     validation$original %>% select(survived, pclass, sex, ageclass, familysize))
```
We find that the optimal threshold is `r logistic_intuition$thresh`. Plotting the ROC curve:

```{r}
plot_ROC_curve(logistic_intuition$analysis, logistic_intuition$threshold)
```

Recall that the confusion matrix gives us the following information:

**Prediction\\Reference** **Yes**  **No**
------------------------- -------  ------
**Yes**                   TP        FP
**No**                    FN        TN

Also: 
* Accuracy = (TP+TN)/(TP + FP + FN + TN)
* Sensitivity  = TP/(TP + FN) - when it's actually yes, how often does the model predict yes? 
True positive, recall
* Specificity = TN/(TN + FP) - When it's actually no, how often does the model predict no?
1-False positive


**Metric**   **Value**
-----------  ----------
Accuracy     `r logistic_intuition$perf['Accuracy']`
Sensitivity  `r logistic_intuition$perf['Sensitivity']`
Specificity  `r logistic_intuition$perf['Specificity']`

In this case, we obtained an accuracy of `r logistic_intuition$perf['Accuracy']`, which is prety good, but can we do better? In the next section we will follow a more data-driven approach to tuning the model.

```{r}
plot(varImp(logistic_intuition$model, scale = FALSE))
```

### Model based on all available variables
We begin by using all available variables in the model:
```{r warning=FALSE}
logistic_full <- evaluate_model(training$original %>% select(-passengerid), 
                                     validation$original %>% select(-passengerid))
```
```{r}
plot_ROC_curve(logistic_full$analysis, logistic_intuition$threshold)
```
We received a number of warnings about rank-deficient prediction. This is due to variables being co-linear. Caret provides information about the relative importance of each variable in the model so we can see which variables play the biggest role.
```{r}
plot(varImp(logistic_full$model, scale = FALSE))
```

##Support Vector Machine

will do this in 2 steps, use caret to tune the parameters
