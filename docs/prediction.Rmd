---
title: "Predicting Passenger Survival"
author: "Richard Kublik"
date: "November 29, 2017"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: 
      collapsed: false
      smooth_scroll: true
---

<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
      ga('create', 'UA-75601650-1', 'auto');
      ga('send', 'pageview');
      
</script>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

# Introduction

As an introductory data science project, I have chosen to explore the data provided by the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition hosted by Kaggle. The competition is to build the best model that can predict whether a given passenger survived the sinking of the Titanic. As a first step, I performed [introductory data analysis](http://http://portfolio.richard.crkublik.com/Titanic/output/exploratory.html) to learn more about the passengers on board. In this second part, I will compare different machine learning algorithms and submit my solution to Kaggle.

We begin by loading the required packages, and performing the data munging steps described in Part 1. For the current prediction task, we are given a training dataset, and a testing dataset. Both datasets have missing data, and we will combine them prior to performing the data munging steps detailed in part 1.

```{r include=FALSE, results="hide"}
library(plyr)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(ranger)
library(e1071)
library(caTools)
library(pROC)
library(randomForest)
library(glmnet)
library(knitr)
library(kernlab)
library(party)
library(ggmosaic)
library(ggbiplot)

# Set seed for reproducible results
set.seed(234233343)

# load the training dataset
train_data <- read_csv(file.path('..','data','train.csv'))
test_data <- read_csv(file.path('..','data','test.csv'))

# Preprocessing, data cleanup the same as for exploratory analysis
# initial pre-processing:
train_data <- train_data %>% 
  mutate(Survived = factor(Survived, levels = c(1, 0), labels = c("yes", "no")))

titanic_data <- train_data %>% 
  bind_rows(test_data) %>% 
  mutate(Pclass = factor(Pclass), 
         Sex = factor(Sex),
         Embarked = factor(Embarked),
         Title = factor(str_extract(Name, "[a-zA-z]+\\.")),
         FamilyName = str_extract(Name, "[a-zA-z]*"))

# Convert variable names to lowercase
names(titanic_data) <- tolower(names(titanic_data))

# Fill in missing values:

# CABIN
titanic_data <- titanic_data %>% select(-cabin) 

# FARE
missing_fare <- titanic_data %>% 
  filter(is.na(fare))
mean_fare <- titanic_data %>% 
  filter(pclass == missing_fare$pclass,
         embarked == missing_fare$embarked,
         sex == missing_fare$sex) %>% 
  summarize(mean_fare = mean(fare, na.rm = TRUE))
print(mean_fare)

titanic_data$fare[which(titanic_data$passengerid == missing_fare$passengerid)] <- mean_fare$mean_fare[1]

# EMBARKED
titanic_data$embarked[which(is.na(titanic_data$embarked))] <- "S"

# AGE
tb <- cbind(titanic_data$age, titanic_data$title)

# get the mean ages for each title
(age_dist <- titanic_data %>% 
  group_by(title) %>% 
  summarize(n = n(),
            n_missing = sum(is.na(age)),
            perc_missing = 100*n_missing/n,
            mean_age = mean(age, na.rm = TRUE),
            sd_age = sd(age, na.rm = TRUE),
            min_age = min(age, na.rm = TRUE),
            max_age = max(age, na.rm = TRUE)) %>% 
  filter(n_missing > 0))

titanic_data <- titanic_data %>% 
  mutate(age_est = 0)


bnorm <- function(nsamp, mean, std, lbound, ubound, rounding){
  samp <- round(rnorm(nsamp, mean, std), rounding)
  samp[samp < lbound] <- lbound
  samp[samp > ubound] <- ubound
  samp
}

for (key in c("Dr.", "Master.", "Miss.", "Mr.", "Mrs.")) {
  idx_na <- which(titanic_data$title == key & is.na(titanic_data$age))
  age_idx <- which(age_dist$title == key)
  titanic_data$age[idx_na] <- bnorm(length(idx_na), 
                                    age_dist$mean_age[age_idx], 
                                    age_dist$sd_age[age_idx],
                                    age_dist$min_age[age_idx],
                                    age_dist$max_age[age_idx],
                                    1)
  titanic_data$age_est[idx_na] <- 1
}
# impute single missing Ms. value to be the mean:
idx_na <- which(titanic_data$title == "Ms." & is.na(titanic_data$age))
age_idx <- which(age_dist$title == "Ms.")
titanic_data$age[idx_na] <- age_dist$mean_age[age_idx]
titanic_data$age_est[idx_na] <- 1


```

# Further Data Cleanup

## Passenger Titles
We have already seen that there are many different titles assigned to the passengers:
```{r echo = FALSE}
titanic_data %>% 
  group_by(title, sex) %>% 
  summarize(Num_Passengers = n()) %>% 
  kable()
```

While these titles were informative, they add a level of complexity that may have a detremental effect on the machine learning models that we will be looking at. We will reduce the number of titles to 4: Mr., Mrs., Master., and Miss. These titles capture both gender and age information.

```{r }
titanic_data <- titanic_data %>% 
  mutate(title = as.character(title),
         title = ifelse((title == "Dr.") & sex == "female", "Mrs.", title),
         title = ifelse(title == "Mlle.", "Miss.", title),
         title = ifelse((title != "Miss.") & sex == "female", "Mrs.", title),
         title = ifelse((title != "Master.") & sex == "male", "Mr.", title),
         title = factor(title))
```

Having finished the data munging steps, we will again divide our data into the training and testing datasets.
```{r}
test_idx <- which(is.na(titanic_data$survived))
training_data <- titanic_data[-test_idx,]
testing_data <- titanic_data[test_idx,]
```

# Data Exploration
Before building our predictive model, we want to explore the data and see which variables might be most important. We begin by examining the impact of each variable on the survival rate.
```{r echo = FALSE}
mosaic_plot <- function(data, varx, vary){
  stats <- data %>%
    count_(varx)
    
  total_rows <- dim(data)[1]
  data$weight = 0

  for (s in 1:dim(stats)[1]) {
    data$weight[which(data[[varx]] == stats[[1]][[s]])] = stats$n[[s]]/total_rows
  }
  
  data %>% 
    ggplot() +
    geom_mosaic(aes(weight = weight, x = product(get(vary), get(varx)), fill = get(vary))) +
    labs(x = varx,
         y = "Percent",
         title = sprintf("%s vs %s", vary, varx)) + 
    scale_fill_discrete(name = vary)
}
```
<div class="row">
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    mosaic_plot(training_data,"pclass","survived") 
```
  </div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"sex","survived")  
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"sibsp","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"parch","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"embarked","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"title","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo = FALSE}
training_data %>% 
    ggplot(aes_string(x = "survived", y = "age")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Survial",
       y = "Age",
       title = "Age distribution by survival")
```
</div>
  <div class="col-md-4 col-sm-6">
  ```{r echo=FALSE}
training_data %>% 
    ggplot(aes_string(x = "survived", y = "fare")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Survial",
       y = "Fare",
       title = "Fare distribution by survival")
```
</div>
</div>

From these plots, it appears that *pclass*, *sex*, and *title* have strong correlations with survival, while *sibsp*, *parch*, and *embarked* have a lesser (though noticible) correlation. 

The distributions of *age* and *fare* do not appear significantly different for the suviving passengers and those that perished. As we saw previously, there are a number of passengers in 3rd class who paid more for their tickets than some first class passengers. However once onboard passengers would have been treated according to their travel class.

# Predictive Models
Using the insight we have gained in the previous section, we will construct a predictive model, and submit our predictions to the Kaggle competition. We begin by splitting the training dataset into training and validation sets that will help evaluate the performance of our models.

```{r}
train_idx <- createDataPartition(training_data$survived, p = 0.7, list = FALSE)

train <- training_data[train_idx,]
validation <- training_data[-train_idx,]

train_control <- trainControl(method = "none",
                              classProbs = TRUE)
data  <- list("train" = train, "test" = validation)
```

## Logistic Regression
To reduce the chances of over fitting, we begin with the simplest classification model: logistic regression. 
```{r message = FALSE, warning = FALSE}
# create a function to run the logistic regression for us.
evaluate_model <- function(train_data,test_data){
  
  #train the model
  model <-  train(survived ~ .,
                  data = train_data,
                  method = "glm",
                  metric = "ROC", 
                  trControl = train_control,
                  family = binomial(link = "logit"))
  
  pred <-  predict(model, test_data, type = "prob")
  analysis <- roc(response = test_data$survived, predictor = pred$yes)
  error <- cbind(analysis$thresholds, analysis$sensitivities + analysis$specificities)
  thresh <- subset(error, error[,2] == max(error[,2]))[,1]
  

  # determine confusion matrix for testing data
  # Turn probabilities into classes, and look at frequencies:
  #Because the logistic regression model provides probabilities of a passenger surviving or perishing, 
  #we need to determine the probability threshold that will be used to classify passengers.

  pred$survived <- ifelse(pred[['yes']] > thresh, 1, 0)
  pred$survived <- factor(pred$survived, levels = c(1, 0), labels = c("yes", "no"))
  conf <-  confusionMatrix(pred$survived, test_data$survived)

  
  perf <- c(conf$overall["Accuracy"],
            conf$byClass["Sensitivity"],
            conf$byClass["Specificity"])
  
  print(conf$table)
  print(perf)
  
  list('model' = model,
       'prediction' = pred,
       'analysis' = analysis,
       'threshold' = thresh,
       'conf' = conf,
       'perf' = perf
       )
}

plot_ROC_curve <- function(analysis, thresh){
    #Plot ROC Curve
  plot(1 - analysis$specificities,analysis$sensitivities,type = "l",
       ylab = "Sensitiviy",
       xlab = "1-Specificity",
       col = "black",
       lwd = 2,
       main = "ROC Curve for Simulated Data")
  abline(a = 0,b = 1)
  abline(v = thresh) #add optimal t to ROC curve
}

```
We begin by including all the variables we determined to be very or somewhat important. We exclude the sex variable as the same information is encoded in title.
```{r}
#evaluate model 1:
results1 <- evaluate_model(data$train %>% select(survived, pclass, sibsp, parch, embarked, title),
                           data$test %>% select(survived, pclass, sibsp, parch, embarked, title))
```


We find that the optimal threshold is `r results1$thresh`. We can plot the ROC curve:
```{r}
plot_ROC_curve(results1$analysis, results1$threshold)
```


Recall that the confusion matrix gives us the following information:

**Prediction\\Reference** **Yes**  **No**
------------------------- -------  ------
**Yes**                   TP        FP
**No**                    FN        TN

Also: 
* Accuracy = (TP+TN)/(TP + FP + FN + TN)
* Sensitivity  = TP/(TP + FN) - when it's actually yes, how often does the model predict yes? 
True positive, recall
* Specificity = TN/(TN + FP) - When it's actually no, how often does the model predict no?
1-False positive


**Metric**   **Value**
-----------  ----------
Accuracy     `r results1$perf['Accuracy']`
Sensitivity  `r results1$perf['Sensitivity']`
Specificity  `r results1$perf['Specificity']`

In this case, we obtained an accuracy of `r results1$perf['Accuracy']`, which is prety good, but can we do better? 
In general there are two ways to improve the accuracy of a model:
* Create a more complex model by including more features, or using a more sophisticated algorithm
* Provide more training data

We can determine which option will provide the best approach by plotting the learning curves for our model:

```{r}
calc_accuracy <- function(pred, label){
  analysis <- roc(response = label, predictor = pred$yes)
  error <- cbind(analysis$thresholds, analysis$sensitivities + analysis$specificities)
  thresh <- subset(error, error[,2] == max(error[,2]))[,1]


  pred$survived <- ifelse(pred[['yes']] > thresh, 1, 0)
  pred$survived <- factor(pred$survived, levels = c(1, 0), labels = c("yes", "no"))
  conf <-  confusionMatrix(pred$survived, label)
  
  accuracy <- conf$overall["Accuracy"]
  accuracy
}

model_accuracy <- function(train_data, cv_data){
  model <- train(survived ~ .,
                  data = train_data,
                  method = "glm",
                  metric = "ROC",
                  trControl = train_control,
                  family = binomial(link = "logit"))

  train_pred <- predict(model, train_data, type = "prob")
  train_accuracy <- calc_accuracy(train_pred, train_data$survived)
  
  cv_pred <- predict(model, cv_data, type = "prob")
  cv_accuracy <- calc_accuracy(cv_pred, cv_data$survived)
  
  data <- list("train" = train_accuracy, "cv" = cv_accuracy)
  data
}


calc_learning_curves <-  function(train, test){

  nrows <- c(2:nrow(train))
  train_accuracy <- rep(0, length(nrows))
  cv_accuracy <- rep(0, length(nrows))

  for (i in 1:length(nrows)) {
    adata <-  model_accuracy(train %>% head(nrows[i]), test)
    train_accuracy[i] <- adata$train
    cv_accuracy[i] <- adata$cv
  }

  data.frame(cbind(nrows, train_accuracy, cv_accuracy))
}

plot_learning_curves <- function(plotdf){
    ggplot(data = plotdf, aes(x = nrows)) +
      geom_line(aes(y = train_accuracy, color = "Training"), size = 1) + 
      geom_line(aes(y = cv_accuracy, color = "Validation"), size = 1) +
      xlab("Size of Training Set") +
      ylab("Accuracy (%)") +
      ggtitle("Learning Curves for Logistic Regression") +
      theme(legend.title = element_blank())
}

```

```{r}
model1_learning <- calc_learning_curves(data$train %>% select(survived, pclass, sibsp, parch, embarked, title),
                                        data$test %>% select(survived, pclass, sibsp, parch, embarked, title))

plot_learning_curves(model1_learning)

```

From the learning curves, we see that adding more complexity to the model may improve it's performance.
Let's start by looking at the false positives, and false negatives and see if we can find a pattern:

**Do I need to scale the data before using logistic regression?**
I suspect I do. look at how I need to transform the data before continuing....
look at what variables were used:
```{r}
imp <- varImp(results1$model, scale = FALSE)
plot(imp)
```
So, Caret automatically converts the categorical variables to a set of binary variables.
It appears that travel class and title are the most important variables.

Now, look at the false positives and false negatives, what can we learn:
```{r}
results1$comp <- data$test %>% 
  cbind(prediction = results1$pred$survived)
#False positives:
print('False positives')
results1$comp %>% 
  filter(prediction == "yes" & prediction != survived) %>% 
  #select(pclass, sex, sibsp,parch ,embarked ,title, prediction, survived) %>% 
  summary()
#False negatives:
print("False negatives")
results1$comp %>% 
  filter(prediction == "no" & prediction != survived) %>% 
  #select(pclass, sex, sibsp,parch ,embarked ,title, prediction, survived) %>% 
  summary()

print("All misclassified")
results1$comp %>% 
  filter(prediction != survived) %>% 
  #select(pclass, sex, sibsp,parch ,embarked ,title, prediction, survived) %>% 
  summary()

```
As an aside.. Got warnings about rank-deficient prediction, do we have any variables that are colinear?
```{r}
train_pca <- titanic_data %>% 
  select(pclass, sex, sibsp,parch ,embarked ,title) %>% 
  mutate(pclass = as.numeric(pclass),
         sex = as.numeric(sex),
         embarked = as.numeric(embarked),
         title = as.numeric(title)) %>% 
  prcomp(center = TRUE, scale = TRUE)

ggbiplot(train_pca)
```
We see that on the full dataset, embarked and pclass are colinear in the pca space... why is that? make a mosaicplot:
```{r}
mosaic_plot(training_data,"pclass","embarked")
```


## Add age information:
In our first attempt, we used the title variable as a proxy for both age and sex. In this first expansion, we will create a categorical variable for age with the categories: child, teen, adult, senior. To get a sense of where the category splits should be, we look at the histogram of ages:
```{r}
titanic_data %>% 
  ggplot(aes(x = age, fill = factor(age_est, levels = c(1, 0)))) + 
  geom_histogram(binwidth = 1) +
  geom_rug() +
  ggtitle('Age distribution of passengers') +
  scale_fill_discrete(name = "Age",
                      breaks = c("0", "1"),
                      labels = c("Reported", "Estimated")) +
  labs(x = "Age", 
       y = "Number of Passengers")
```
We will set the range of ages: child: < 10, teen 11-19, adult 20-60, senior >60
```{r}
#make a function, so it's easy to apply it to the test data at the end.
add_age_category <- function(data){
  data %>%  
    mutate(ageclass = ifelse(age < 10, 'child',
                           ifelse(age < 20, 'teen',
                                  ifelse(age < 60, 'adult', 'senior')
                                  )
                           )
         ) %>% 
  mutate(ageclass = as.factor(ageclass))
}
training_data <- training_data %>% add_age_category()
# push changes through to train/test sets:
train <- training_data[train_idx,]
validation <- training_data[-train_idx,]

data  <- list("train" = train, "test" = validation)

```
Now, let's see if this had any effect:
```{r}
results2 <- evaluate_model(data$train %>% select(survived, pclass, sibsp, parch, embarked, sex, ageclass),
                           data$test %>% select(survived, pclass, sibsp, parch, embarked, sex, ageclass))
imp <- varImp(results2$model, scale = FALSE)
plot(imp)
```
Hmm... less accurate. Let's add a family size variable:
```{r}
#make a function, so it's easy to apply it to the test data at the end.
add_family_size <- function(data){
  data %>%  
    mutate(familysize = 1 + sibsp + parch)
}
training_data <- training_data %>% add_family_size()
# push changes through to train/test sets:
train <- training_data[train_idx,]
validation <- training_data[-train_idx,]

data  <- list("train" = train, "test" = validation)

```
Now, let's see if this had any effect:
```{r}
results3 <- evaluate_model(data$train %>% select(survived, pclass, sibsp, parch, embarked, sex, ageclass, familysize),
                           data$test %>% select(survived, pclass, sibsp, parch, embarked, sex, ageclass, familysize))
imp <- varImp(results3$model, scale = FALSE)
plot(imp)
```
