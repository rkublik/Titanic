---
title: "Predicting Passenger Survival"
author: "Richard Kublik"
date: "November 29, 2017"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: 
      collapsed: false
      smooth_scroll: true
---

<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
      ga('create', 'UA-75601650-1', 'auto');
      ga('send', 'pageview');
      
</script>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

# Introduction

As an introductory data science project, I have chosen to explore the data provided by the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition hosted by Kaggle. The competition is to build the best model that can predict whether a given passenger survived the sinking of the Titanic. As a first step, I performed [introductory data analysis](http://http://portfolio.richard.crkublik.com/Titanic/output/exploratory.html) to learn more about the passengers on board. In this second part, I will compare different machine learning algorithms and submit my solution to Kaggle.

We begin by loading the required packages, and performing the data munging steps described in Part 1. For the current prediction task, we are given a training dataset, and a testing dataset. Both datasets have missing data, and we will combine them prior to performing the data munging steps detailed in part 1.

```{r include=FALSE, results="hide"}
library(plyr)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(ranger)
library(e1071)
library(caTools)
library(pROC)
library(randomForest)
library(glmnet)
library(knitr)
library(kernlab)
library(party)
library(ggmosaic)
library(ggbiplot)

# Set seed for reproducible results
set.seed(234233343)

# load the training dataset
train_data <- read_csv(file.path('..','data','train.csv'))
test_data <- read_csv(file.path('..','data','test.csv'))

# Preprocessing, data cleanup the same as for exploratory analysis
# initial pre-processing:
train_data <- train_data %>% 
  mutate(Survived = factor(Survived, levels = c(1, 0), labels = c("yes", "no")))

titanic_data <- train_data %>% 
  bind_rows(test_data) %>% 
  mutate(Pclass = factor(Pclass), 
         Sex = factor(Sex),
         Embarked = factor(Embarked),
         Title = factor(str_extract(Name, "[a-zA-z]+\\.")),
         FamilyName = str_extract(Name, "[a-zA-z]*"))

# Convert variable names to lowercase
names(titanic_data) <- tolower(names(titanic_data))

# Fill in missing values:

# CABIN
# find shared tickets:
ticket_deck <-  titanic_data %>% 
  filter(!is.na(cabin)) %>% 
  distinct(ticket, .keep_all = TRUE) %>% 
  select(ticket, cabin) %>% 
  mutate(deck = substr(cabin, 0, 1)) %>% 
  select(-cabin)
  

# fill in missing deck from same ticket
titanic_data <- titanic_data %>% 
  merge(ticket_deck, by = "ticket", all.x = TRUE) %>% 
  mutate(tdeck = substr(cabin, 0, 1),
         deck = ifelse(!is.na(tdeck), tdeck, deck),
         deck = ifelse(is.na(deck), "U", deck)) %>% 
  select(-tdeck) 



# FARE
missing_fare <- titanic_data %>% 
  filter(is.na(fare))
mean_fare <- titanic_data %>% 
  filter(pclass == missing_fare$pclass,
         embarked == missing_fare$embarked,
         sex == missing_fare$sex) %>% 
  summarize(mean_fare = mean(fare, na.rm = TRUE))
print(mean_fare)

titanic_data$fare[which(titanic_data$passengerid == missing_fare$passengerid)] <- mean_fare$mean_fare[1]

# EMBARKED
titanic_data$embarked[which(is.na(titanic_data$embarked))] <- "S"

# AGE
tb <- cbind(titanic_data$age, titanic_data$title)

# get the mean ages for each title
(age_dist <- titanic_data %>% 
  group_by(title) %>% 
  summarize(n = n(),
            n_missing = sum(is.na(age)),
            perc_missing = 100*n_missing/n,
            mean_age = mean(age, na.rm = TRUE),
            sd_age = sd(age, na.rm = TRUE),
            min_age = min(age, na.rm = TRUE),
            max_age = max(age, na.rm = TRUE)) %>% 
  filter(n_missing > 0))

titanic_data <- titanic_data %>% 
  mutate(age_est = 0)


bnorm <- function(nsamp, mean, std, lbound, ubound, rounding){
  samp <- round(rnorm(nsamp, mean, std), rounding)
  samp[samp < lbound] <- lbound
  samp[samp > ubound] <- ubound
  samp
}

for (key in c("Dr.", "Master.", "Miss.", "Mr.", "Mrs.")) {
  idx_na <- which(titanic_data$title == key & is.na(titanic_data$age))
  age_idx <- which(age_dist$title == key)
  titanic_data$age[idx_na] <- bnorm(length(idx_na), 
                                    age_dist$mean_age[age_idx], 
                                    age_dist$sd_age[age_idx],
                                    age_dist$min_age[age_idx],
                                    age_dist$max_age[age_idx],
                                    1)
  titanic_data$age_est[idx_na] <- 1
}
# impute single missing Ms. value to be the mean:
idx_na <- which(titanic_data$title == "Ms." & is.na(titanic_data$age))
age_idx <- which(age_dist$title == "Ms.")
titanic_data$age[idx_na] <- age_dist$mean_age[age_idx]
titanic_data$age_est[idx_na] <- 1


```

# Further Data Cleanup

## Passenger Titles
We have already seen that there are many different titles assigned to the passengers:
```{r echo = FALSE}
titanic_data %>% 
  group_by(title, sex) %>% 
  summarize(Num_Passengers = n()) %>% 
  kable()
```

While these titles were informative, they add a level of complexity that may have a detremental effect on the machine learning models that we will be looking at. We will reduce the number of titles to 4: Mr., Mrs., Master., and Miss. These titles capture both gender and age information.

```{r }
titanic_data <- titanic_data %>% 
  mutate(title = as.character(title),
         title = ifelse((title == "Dr.") & sex == "female", "Mrs.", title),
         title = ifelse(title == "Mlle.", "Miss.", title),
         title = ifelse((title != "Miss.") & sex == "female", "Mrs.", title),
         title = ifelse((title != "Master.") & sex == "male", "Mr.", title),
         title = factor(title))
```

Having finished the data munging steps, we will again divide our data into the training and testing datasets.
```{r}
test_idx <- which(is.na(titanic_data$survived))
training_data <- titanic_data[-test_idx,]
testing_data <- titanic_data[test_idx,]
```

# Data Exploration
Before building our predictive model, we want to explore the data and see which variables might be most important. We begin by examining the impact of each variable on the survival rate.
```{r echo = FALSE}
mosaic_plot <- function(data, varx, vary){
  stats <- data %>%
    count_(varx)
    
  total_rows <- dim(data)[1]
  data$weight = 0

  for (s in 1:dim(stats)[1]) {
    data$weight[which(data[[varx]] == stats[[1]][[s]])] = stats$n[[s]]/total_rows
  }
  
  data %>% 
    ggplot() +
    geom_mosaic(aes(weight = weight, x = product(get(vary), get(varx)), fill = get(vary))) +
    labs(x = varx,
         y = "Percent",
         title = sprintf("%s vs %s", vary, varx)) + 
    scale_fill_discrete(name = vary)
}
```
<div class="row">
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    mosaic_plot(training_data,"pclass","survived") 
```
  </div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"sex","survived")  
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"sibsp","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"parch","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"embarked","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"title","survived")
```
</div>
<div class="col-md-4 col-sm-6">
```{r echo=FALSE}
mosaic_plot(training_data,"deck","survived")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo = FALSE}
training_data %>% 
    ggplot(aes_string(x = "survived", y = "age")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Survial",
       y = "Age",
       title = "Age distribution by survival")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
training_data %>% 
    ggplot(aes_string(x = "survived", y = "fare")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Survial",
       y = "Fare",
       title = "Fare distribution by survival")
```
</div>
</div>

From these plots, it appears that *pclass*, *sex*, *deck*, and *title* have strong correlations with survival, while *sibsp*, *parch*, and *embarked* have a lesser (though noticible) correlation. 

The distributions of *age* and *fare* do not appear significantly different for the suviving passengers and those that perished. As we saw previously, there are a number of passengers in 3rd class who paid more for their tickets than some first class passengers. However once onboard passengers would have been treated according to their travel class.

# Predictive Models
Using the insight we have gained in the previous section, we will construct a predictive model, and submit our predictions to the Kaggle competition. We begin by splitting the training dataset into training and validation sets that will help evaluate the performance of our models.

```{r}
train_idx <- createDataPartition(training_data$survived, p = 0.7, list = FALSE)

train <- training_data[train_idx,]
validation <- training_data[-train_idx,]

train_control <- trainControl(method = "none",
                              classProbs = TRUE)
data  <- list("train" = train, "test" = validation)
```

### Logistic Regression
To reduce the chances of over fitting, we begin with the simplest classification model: logistic regression. 
```{r message = FALSE, warning = FALSE}
# create a function to run the logistic regression for us.
evaluate_model <- function(train_data,test_data, interactions = FALSE){
  
  #train the model
  if (interactions) {
  model <-  train(survived ~ .*.,
                  data = train_data,
                  method = "glm",
                  metric = "ROC", 
                  trControl = train_control,
                  family = binomial(link = "logit"))
  } else {
    
  model <-  train(survived ~ .,
                  data = train_data,
                  method = "glm",
                  metric = "ROC", 
                  trControl = train_control,
                  family = binomial(link = "logit"))
  }
  
  pred <-  predict(model, test_data, type = "prob")
  analysis <- roc(response = test_data$survived, predictor = pred$yes)
  error <- cbind(analysis$thresholds, analysis$sensitivities + analysis$specificities)
  thresh <- subset(error, error[,2] == max(error[,2]))[,1]
  

  # determine confusion matrix for testing data
  # Turn probabilities into classes, and look at frequencies:
  #Because the logistic regression model provides probabilities of a passenger surviving or perishing, 
  #we need to determine the probability threshold that will be used to classify passengers.

  pred$survived <- ifelse(pred[['yes']] > thresh, 1, 0)
  pred$survived <- factor(pred$survived, levels = c(1, 0), labels = c("yes", "no"))
  conf <-  confusionMatrix(pred$survived, test_data$survived)

  
  perf <- c(conf$overall["Accuracy"],
            conf$byClass["Sensitivity"],
            conf$byClass["Specificity"])
  
  print(conf$table)
  print(perf)
  
  list('model' = model,
       'prediction' = pred,
       'analysis' = analysis,
       'threshold' = thresh,
       'conf' = conf,
       'perf' = perf
       )
}

plot_ROC_curve <- function(analysis, thresh){
    #Plot ROC Curve
  plot(1 - analysis$specificities,analysis$sensitivities,type = "l",
       ylab = "Sensitiviy",
       xlab = "1-Specificity",
       col = "black",
       lwd = 2,
       main = "ROC Curve for Simulated Data")
  abline(a = 0,b = 1)
  abline(v = thresh) #add optimal t to ROC curve
}

```
We begin by including all the variables we determined to be very or somewhat important. We exclude the sex variable as the same information is encoded in title.
```{r}
#evaluate model 1:
logistic1 <- evaluate_model(data$train %>% select(survived, pclass, sibsp, parch, embarked, title),
                           data$test %>% select(survived, pclass, sibsp, parch, embarked, title))
```


We find that the optimal threshold is `r logistic1$thresh`. We can plot the ROC curve:
```{r}
plot_ROC_curve(logistic1$analysis, logistic1$threshold)
```


Recall that the confusion matrix gives us the following information:

**Prediction\\Reference** **Yes**  **No**
------------------------- -------  ------
**Yes**                   TP        FP
**No**                    FN        TN

Also: 
* Accuracy = (TP+TN)/(TP + FP + FN + TN)
* Sensitivity  = TP/(TP + FN) - when it's actually yes, how often does the model predict yes? 
True positive, recall
* Specificity = TN/(TN + FP) - When it's actually no, how often does the model predict no?
1-False positive


**Metric**   **Value**
-----------  ----------
Accuracy     `r logistic1$perf['Accuracy']`
Sensitivity  `r logistic1$perf['Sensitivity']`
Specificity  `r logistic1$perf['Specificity']`

In this case, we obtained an accuracy of `r logistic1$perf['Accuracy']`, which is prety good, but can we do better? 
In general there are two ways to improve the accuracy of a model:
* Create a more complex model by including more features, or using a more sophisticated algorithm
* Provide more training data

We can determine which option will provide the best approach by plotting the learning curves for our model:

```{r}
calc_accuracy <- function(pred, label){
  analysis <- roc(response = label, predictor = pred$yes)
  error <- cbind(analysis$thresholds, analysis$sensitivities + analysis$specificities)
  thresh <- subset(error, error[,2] == max(error[,2]))[,1]


  pred$survived <- ifelse(pred[['yes']] > thresh, 1, 0)
  pred$survived <- factor(pred$survived, levels = c(1, 0), labels = c("yes", "no"))
  conf <-  confusionMatrix(pred$survived, label)
  
  accuracy <- conf$overall["Accuracy"]
  accuracy
}

model_accuracy <- function(train_data, cv_data){
  model <- train(survived ~ .,
                  data = train_data,
                  method = "glm",
                  metric = "ROC",
                  trControl = train_control,
                  family = binomial(link = "logit"))

  train_pred <- predict(model, train_data, type = "prob")
  train_accuracy <- calc_accuracy(train_pred, train_data$survived)
  
  cv_pred <- predict(model, cv_data, type = "prob")
  cv_accuracy <- calc_accuracy(cv_pred, cv_data$survived)
  
  list("train" = train_accuracy, "cv" = cv_accuracy)
}


calc_learning_curves <-  function(train, test){

  nrows <- c(2:nrow(train))
  train_accuracy <- rep(0, length(nrows))
  cv_accuracy <- rep(0, length(nrows))

  for (i in 1:length(nrows)) {
    adata <-  model_accuracy(train %>% head(nrows[i]), test)
    train_accuracy[i] <- adata$train
    cv_accuracy[i] <- adata$cv
  }

  data.frame(cbind(nrows, train_accuracy, cv_accuracy))
}

plot_learning_curves <- function(plotdf, model){
    ggplot(data = plotdf, aes(x = nrows)) +
      geom_line(aes(y = 1 - train_accuracy, color = "Training"), size = 1) + 
      geom_line(aes(y = 1 - cv_accuracy, color = "Validation"), size = 1) +
      xlab("Size of Training Set") +
      ylab("Error (%)") +
      ggtitle(paste("Learning Curves for", model)) +
      theme(legend.title = element_blank())
}

```

```{r include = FALSE}
#model1_learning <- calc_learning_curves(data$train %>% select(survived, pclass, sibsp, parch, embarked, title),
#                                        data$test %>% select(survived, pclass, sibsp, parch, embarked, title))
#
#plot_learning_curves(model1_learning, "Logistic Regression model 1")
```

From the learning curves, we see that adding more complexity to the model may improve it's performance.
Let's start by looking at the false positives, and false negatives and see if we can find a pattern:

**Do I need to scale the data before using logistic regression?**
I suspect I do. look at how I need to transform the data before continuing....
look at what variables were used:
```{r}
imp <- varImp(logistic1$model, scale = FALSE)
plot(imp)
```
So, Caret automatically converts the categorical variables to a set of binary variables.
It appears that travel class and title are the most important variables.

### Exploration of misclassified data points

```{r echo = FALSE}
logistic1$comp <- data$test %>% 
  cbind(prediction = logistic1$pred$survived) %>% 
  mutate(misclassified = survived != prediction,
         misclassification = ifelse(misclassified == TRUE & prediction == 'yes', 'false_pos',
                                    ifelse(misclassified == TRUE & prediction == 'no', 'false_neg', 'correct')),
         misclassification = as.factor(misclassification))
  

mosaic_facet <- function(data, varx, vary, facet){
  stats <- data %>%
    count_(varx)
    
  total_rows <- dim(data)[1]
  data$weight = 0

  for (s in 1:dim(stats)[1]) {
    data$weight[which(data[[varx]] == stats[[1]][[s]])] = stats$n[[s]]/total_rows
  }
  
  data %>% 
    ggplot() +
    geom_mosaic(aes(weight = weight, x = product(get(vary), get(varx)), fill = get(vary))) +
    labs(x = varx,
         y = "Percent",
         title = sprintf("%s vs %s", vary, varx)) + 
    scale_fill_discrete(name = vary) +
    facet_wrap(~ get(facet))
}
```
#### mosaic plots of misclassification

<div class="row">
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    logistic1$comp %>%  mosaic_plot( "misclassified", "pclass")  

```
  </div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    logistic1$comp %>%  mosaic_plot( "misclassified", "sex")  

```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    logistic1$comp %>%  mosaic_plot( "misclassified", "sibsp")  

```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    logistic1$comp %>%  mosaic_plot( "misclassified", "parch")  

```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    logistic1$comp %>%  mosaic_plot( "misclassified", "embarked")  

```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    logistic1$comp %>%  mosaic_plot( "misclassified", "title")  

```
</div>
<div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    logistic1$comp %>%  mosaic_plot( "misclassified", "deck")  

```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo = FALSE}
logistic1$comp %>% 
    ggplot(aes_string(x = "misclassified", y = "age")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Misclassified",
       y = "Age",
       title = "Age distribution by classification status")
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
logistic1$comp %>% 
    ggplot(aes_string(x = "misclassified", y = "fare")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Misclassified",
       y = "Fare",
       title = "Fare distribution by classification status")
```
</div>
</div>

#### false +/- plots
<div class="row">
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
logistic1$comp %>% 
  ggplot(aes(x = pclass, fill = prediction)) +
  geom_bar() +
  facet_wrap(~ misclassification)
```
  </div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
    logistic1$comp %>% 
  ggplot(aes(x = sex, fill = prediction)) +
  geom_bar() +
  facet_wrap(~ misclassification)
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
   logistic1$comp %>% 
  ggplot(aes(x = sibsp, fill = prediction)) +
  geom_bar() +
  facet_wrap(~ misclassification)
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
logistic1$comp %>% 
  ggplot(aes(x = parch, fill = prediction)) +
  geom_bar() +
  facet_wrap(~ misclassification)
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
logistic1$comp %>% 
  ggplot(aes(x = embarked, fill = prediction)) +
  geom_bar() +
  facet_wrap(~ misclassification)
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
logistic1$comp %>% 
  ggplot(aes(x = title, fill = prediction)) +
  geom_bar() +
  facet_wrap(~ misclassification)
```
</div>
<div class="col-md-4 col-sm-6">
```{r echo=FALSE}
logistic1$comp %>% 
  ggplot(aes(x = deck, fill = prediction)) +
  geom_bar() +
  facet_wrap(~ misclassification)
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo = FALSE}
logistic1$comp %>% 
    ggplot(aes_string(x = "prediction", y = "age")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "predicted survival",
       y = "Age",
       title = "Age distribution by classification status") + 
  facet_wrap(~ misclassification)
```
</div>
  <div class="col-md-4 col-sm-6">
```{r echo=FALSE}
logistic1$comp %>% 
    ggplot(aes_string(x = "prediction", y = "fare")) + 
    geom_boxplot(varwidth = TRUE) +
  labs(x = "Predicted survival",
       y = "Fare",
       title = "Fare distribution by classification status") +
  facet_wrap(~ misclassification)
```
</div>
</div>
### summary of false +/-
Now, look at the false positives and false negatives, what can we learn:
```{r}
#logistic1$comp <- data$test %>% 
#  cbind(prediction = logistic1$pred$survived)
#False positives:
print('False positives')
logistic1$comp %>% 
  filter(prediction == "yes" & prediction != survived) %>% 
  #select(pclass, sex, sibsp,parch ,embarked ,title, prediction, survived) %>% 
  summary()
#False negatives:
print("False negatives")
logistic1$comp %>% 
  filter(prediction == "no" & prediction != survived) %>% 
  #select(pclass, sex, sibsp,parch ,embarked ,title, prediction, survived) %>% 
  summary()

print("All misclassified")
logistic1$comp %>% 
  filter(prediction != survived) %>% 
  #select(pclass, sex, sibsp,parch ,embarked ,title, prediction, survived) %>% 
  summary()

```
As an aside.. Got warnings about rank-deficient prediction, do we have any variables that are colinear?
```{r}
train_pca <- titanic_data %>% 
  select(pclass, sex, sibsp,parch ,embarked ,title) %>% 
  mutate(pclass = as.numeric(pclass),
         sex = as.numeric(sex),
         embarked = as.numeric(embarked),
         title = as.numeric(title)) %>% 
  prcomp(center = TRUE, scale = TRUE)

ggbiplot(train_pca)
```
We see that on the full dataset, embarked and pclass are colinear in the pca space... why is that? make a mosaicplot:
```{r}
mosaic_plot(training_data,"pclass","embarked")
```

### Try random feature engineering....
In our first attempt, we used the title variable as a proxy for both age and sex. In this first expansion, we will create a categorical variable for age with the categories: child, teen, adult, senior. To get a sense of where the category splits should be, we look at the histogram of ages:
```{r}
titanic_data %>% 
  ggplot(aes(x = age, fill = factor(age_est, levels = c(1, 0)))) + 
  geom_histogram(binwidth = 1) +
  geom_rug() +
  ggtitle('Age distribution of passengers') +
  scale_fill_discrete(name = "Age",
                      breaks = c("0", "1"),
                      labels = c("Reported", "Estimated")) +
  labs(x = "Age", 
       y = "Number of Passengers")
```
We will set the range of ages: child: < 10, teen 11-19, adult 20-60, senior >60
```{r}
#make a function, so it's easy to apply it to the test data at the end.
add_age_category <- function(data){
  data %>%  
    mutate(ageclass = ifelse(age < 10, 'child',
                           ifelse(age < 20, 'teen',
                                  ifelse(age < 60, 'adult', 'senior')
                                  )
                           )
         ) %>% 
  mutate(ageclass = as.factor(ageclass))
}
training_data <- training_data %>% add_age_category()
# push changes through to train/test sets:
train <- training_data[train_idx,]
validation <- training_data[-train_idx,]

data  <- list("train" = train, "test" = validation)

```
Now, let's see if this had any effect:
```{r}
logistic2 <- evaluate_model(data$train %>% select(survived, pclass, sibsp, parch, embarked, sex, ageclass),
                           data$test %>% select(survived, pclass, sibsp, parch, embarked, sex, ageclass))
imp <- varImp(logistic2$model, scale = FALSE)
plot(imp)
```
Hmm... less accurate. Let's add a family size variable:
```{r}
#make a function, so it's easy to apply it to the test data at the end.
add_family_size <- function(data){
  data %>%  
    mutate(familysize = 1 + sibsp + parch) 
}
training_data <- training_data %>% add_family_size()
# push changes through to train/test sets:
train <- training_data[train_idx,]
validation <- training_data[-train_idx,]

data  <- list("train" = train, "test" = validation)

```
Now, let's see if this had any effect.we remove the sibsp variable as it parch and sibsp are included in family size
```{r}
logistic3 <- evaluate_model(data$train %>% select(survived, pclass, parch, embarked, sex, age, familysize),
                           data$test %>% select(survived, pclass, parch, embarked, sex, age, familysize), interactions = FALSE)
imp <- varImp(logistic3$model, scale = FALSE)
plot(imp)
```
If we add interactions between the variables, we obtain a more complex model:
```{r}
logistic4 <- evaluate_model(data$train %>% select(survived, pclass, parch, embarked, sex, age, familysize),
                           data$test %>% select(survived, pclass, parch, embarked, sex, age, familysize), interactions = TRUE)
imp <- varImp(logistic4$model, scale = FALSE)
plot(imp)
```

```{r include = FALSE}
#logistic5 <- evaluate_model(data$train %>% select(survived, pclass, parch, embarked, sex, age, familysize,ticket),
#                           data$test %>% select(survived, pclass, parch, embarked, sex, age, familysize, ticket), interactions = FALSE)
#imp <- varImp(logistic5$model, scale = FALSE)
#plot(imp)

#Adding in the ticket, gives 71% accuracy.
```

From our initial exploration, the travel deck seemed to play a role:
```{r}
logistic5 <- evaluate_model(data$train %>% select(survived, pclass, parch, embarked, sex, ageclass, familysize, deck),
                           data$test %>% select(survived, pclass, parch, embarked, sex, ageclass, familysize, deck), 
                           interactions = FALSE)
imp <- varImp(logistic5$model, scale = FALSE)
plot(imp)
```
```{r}
logistic6 <- evaluate_model(data$train %>% select(survived, pclass, sibsp, parch, embarked, title, deck),
                           data$test %>% select(survived, pclass, sibsp, parch, embarked, title, deck), 
                           interactions = FALSE)
imp <- varImp(logistic6$model, scale = FALSE)
plot(imp)
```
```{r}
logistic7 <- evaluate_model(data$train %>% select(survived, pclass, familysize, embarked, title, deck),
                           data$test %>% select(survived, pclass,  familysize, embarked, title, deck), 
                           interactions = FALSE)
imp <- varImp(logistic7$model, scale = FALSE)
plot(imp)
```

## Support Vector Machine
We will continue using the same training and testing partitions for consistency, and will continue using the caret package to run our algorithms

```{r}
library(kernlab)
```
```{r}
svm_train <- data$train %>% 
  select(-ticket, -passengerid, -name, -cabin, -familyname)
svm_test <- data$test %>% 
  select(-ticket, -passengerid, -name, -cabin, -familyname)
```
We will carry out a two pass training and tuning process. In the first pass, we use many of the caret defaults, and tune the parameters in the second pass. *Need to center and scale variables myself....*
```{r}
svm_control <- trainControl(method = "repeatedcv",
                            repeats = 5, 
                            summaryFunction = twoClassSummary,
                            classProbs = TRUE)
svm_tune <- train(survived ~ .,
                  data = svm_train,
                  method = 'svmRadial',
                  tuneLength = 9,
                  #preProc = c("center", "scale"),
                  metric = "ROC",
                  trControl = svm_control)

svm_tune
```
In the second pass, we will use train()'s tuneGrid to do some sensitivity analysis around the values C = 2, and sigma = 0.048 that produced the model with the best ROC value. We use the expand.grid() function to contain all combinations of C and sigma that we want to consider.

```{r}
grid <-  expand.grid(sigma = c(0.43, 0.48, 0.52),
                     C = c(1.75,1.9,2,2.1,2.25))

# train and tune the svm:
svm_tune2 <- train(survived ~ .,
                   data = svm_train,
                   method = "svmRadial",
                   #preProc = c("center", "scale"),
                   metric = "ROC",
                   tuneGrid = grid,
                   trControl = svm_control)

svm_tune2


```
```{r}
svm_pred <-  predict(svm_tune2, svm_test, type = "prob")
svm_pred <- svm_pred %>% 
  mutate(prediction = ifelse(yes > no, "yes", "no"))

svm_comp <- svm_test %>% 
  cbind(prediction = svm_pred$prediction) %>% 
  mutate(correct = survived == prediction)

svm_comp %>% summarize(accuracy = mean(correct))


``` 
## Neural Network
Now, we will consider a multi-layer perceptron neural network and optimize the architecture that will give the highest accuracy on our validation set.
```{r}
# convert data to format for neural network: create dummy variables for categorical, scale and center rest
center_scale <- function(m){
  mean = mean(m)
  std = sd(m)
  (m - mean)/std
}

nn_training <- training_data %>% 
  select(survived, pclass, sex, age, sibsp, parch, fare, embarked, title, deck, ageclass, familysize) %>% 
  mutate(age = center_scale(age),
         sibsp = center_scale(sibsp),
         parch = center_scale(parch),
         fare = center_scale(fare),
         familysize = center_scale(familysize))
nn_dummy <- dummyVars(~ ., data = nn_training)

nndata <- data.frame(predict(nn_dummy, newdata = nn_training)) %>% 
  mutate(survived = (survived.yes)) %>% 
  select(-survived.yes, -survived.no)

nn_train = nndata[train_idx,]
nn_test = nndata[-train_idx,]
```
use the neuralnet package:
```{r}
library(neuralnet)
n = names(nn_train)
f = as.formula(paste("survived ~ ", paste(n[!n %in% "survived"], collapse = " + ")))
#nn <- neuralnet(f, data = nn_train, hidden = 15, linear.output = FALSE)

#plot(nn)
#nn_pred = compute(nn, nn_test %>% select(-survived))
#nn_prediction = nn_pred$net.result %>% round()
#nn_comp = cbind(nn_test, prediction = nn_prediction)
#nn_comp %>% 
#  mutate(correct = survived == prediction) %>% 
#  summarize(accuracy = mean(correct))
```

Now, make some functions:
```{r}
nn_accuracy <- function(model, data){
  prediction <- compute(model, data %>% select(-survived))
  pred <- prediction$net.result %>% round()
  comp <- cbind(data, prediction = pred) %>% 
    mutate(correct = survived == prediction)
  accuracy <- comp %>% summarize(accuracy = mean(correct))
  
  list("com" = comp, "accuracy" = accuracy$accuracy[1])
}

nn_learning_curves <-  function(train, test, step, nhidden){
  nrows <- seq(from = 2, to = nrow(train), by = step)
    #c(2:nrow(train))
  train_accuracy <- rep(0, length(nrows))
  cv_accuracy <- rep(0, length(nrows))
  
  n = names(train)
  f = as.formula(paste("survived ~ ", paste(n[!n %in% "survived"], collapse = " + ")))
    
  for (i in 1:length(nrows)) {
    print(nrows[i])
    nn <- neuralnet(f, data = train %>% head(nrows[i]), hidden = nhidden, linear.output = FALSE, stepmax = 1e8)
    train_accuracy[i] <- nn_accuracy(nn, train %>% head(nrows[i]))$error
    cv_accuracy[i] <- nn_accuracy(nn, test)$error
  }

  data.frame(cbind(nrows, train_error, cv_error))
}

```
```{r}
#nncurves1 <- nn_learning_curves(nn_train, nn_test, 10, 5)
#write.csv(nncurves1, file = 'output/nnet1_learning_curves.csv')
#nncurves1 <- read_csv(file.path('output','nnet1_learning_curves.csv'))
#plot_learning_curves(nncurves1, "Neural Network with 5 hidden nodes")
```
```{r}
#nncurves2 <- nn_learning_curves(nn_train, nn_test, 10, 3)
#write.csv(nncurves2, file = 'output/nnet2_learning_curves.csv')
#nncurves2 <- read_csv(file.path('output','nnet2_learning_curves.csv'))
#plot_learning_curves(nncurves2, "Neural Network with 3 hidden nodes")
```

```{r}
#nncurves3 <- nn_learning_curves(nn_train, nn_test, 10, 2)
#write.csv(nncurves3, file = 'output/nnet3_learning_curves.csv')
#nncurves3 <- read_csv(file.path('output','nnet3_learning_curves.csv'))
#plot_learning_curves(nncurves3, "Neural Network with 2 hidden nodes")
```

```{r}
#nncurves4 <- nn_learning_curves(nn_train, nn_test, 10, 4)
#write.csv(nncurves3, file = 'output/nnet4_learning_curves.csv')
#nncurves4 <- read_csv(file.path('output','nnet4_learning_curves.csv'))
#plot_learning_curves(nncurves4, "Neural Network with 4 hidden nodes")
```
These are not consistent, need to use cross validation to tune the model better...
use caret
```{r}
library(RSNNS, Rcpp)
nn_accuracy <- function(model, data){
pred <- predict(model, data)
comp <- cbind(data, prediction = pred) %>%
  mutate(correct = mean(survived == prediction))
accuracy <- comp %>% 
  summarize(accuracy = mean(correct))
list("comp" = comp, "accuracy" = accuracy$accuracy[1])
}


nn_train <- nn_train %>% mutate(survived = as.factor(survived))
nn_control <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5)
#nn_control <- trainControl(method = "none")
n = names(nn_train)
f = as.formula(paste("survived ~ ", paste(n[!n %in% "survived"], collapse = " + ")))

nn_model <-  caret::train(f,
                    data = nn_train,
                    method = "mlp",
                    tuneGrid = expand.grid(size = c(5,10,15,20)),
                    trControl = nn_control)

nn_accuracy(nn_model, nn_train)$accuracy
nn_accuracy(nn_model, nn_test)$accuracy

nn_model
```
```{r}
nn_model <-  caret::train(f,
                    data = nn_train,
                    method = "mlp",
                    tuneGrid = expand.grid(size = c(10,11,12,13,14,15,16,17,18,19,20)),
                    trControl = nn_control)

nn_accuracy(nn_model, nn_train)$accuracy
nn_accuracy(nn_model, nn_test)$accuracy

nn_model
```
caret selected model with size = 12 as optimal. now get prediction based on that:
```{r}


```

```{r}

nn_learning_curves <-  function(train, test, step, nhidden){
  nrows <- seq(from = 25, to = nrow(train), by = step)
  train_accuracy <- rep(0, length(nrows))
  cv_accuracy <- rep(0, length(nrows))
  
  n = names(train)
  f = as.formula(paste("survived ~ ", paste(n[!n %in% "survived"], collapse = " + ")))
  
  for (i in 1:length(nrows)) {
    print(nrows[i])
    nn <-  caret::train(f,
                    data = train %>% head(nrows[i]),
                    method = "mlp",
                    tuneGrid = expand.grid(size = nhidden),
                    trControl = nn_control)

    train_accuracy[i] <- nn_accuracy(nn, train %>% head(nrows[i]))$accuracy
    cv_accuracy[i] <- nn_accuracy(nn, test)$accuracy
  }

  data.frame(cbind(nrows, train_accuracy, cv_accuracy))
}

```
```{r}
nncurves1 <- nn_learning_curves(nn_train, nn_test, 25, 5)
write.csv(nncurves1, file = 'output/nnet1_learning_curves.csv')
#nncurves1 <- read_csv(file.path('output','nnet1_learning_curves.csv'))
plot_learning_curves(nncurves1, "Neural Network with 5 hidden nodes")
#nnlc <- nn_learning_curves(nn_train, nn_test, 25, 5)
#plot_learning_curves(nnlc, "Neural Network with 5 hidden nodes")
```
```{r}
nncurves2 <- nn_learning_curves(nn_train, nn_test, 25, 15)
write.csv(nncurves2, file = 'output/nnet2_learning_curves.csv')
#nncurves2 <- read_csv(file.path('output','nnet2_learning_curves.csv'))
plot_learning_curves(nncurves2, "Neural Network with 15 hidden nodes")
```
```{r}
nncurves3 <- nn_learning_curves(nn_train, nn_test, 25, 30)
write.csv(nncurves3, file = 'output/nnet3_learning_curves.csv')
#nncurves3 <- read_csv(file.path('output','nnet3_learning_curves.csv'))
plot_learning_curves(nncurves3, "Neural Network with 30 hidden nodes")
```
```{r}
nncurves4 <- nn_learning_curves(nn_train, nn_test, 25, 50)
write.csv(nncurves4, file = 'output/nnet4_learning_curves.csv')
#nncurves4 <- read_csv(file.path('output','nnet4_learning_curves.csv'))
plot_learning_curves(nncurves4, "Neural Network with 50 hidden nodes")
```
```{r}
multilayer_learning_curves <-  function(train, test, step, l1, l2=0, l3=0){
  nrows <- seq(from = 25, to = nrow(train), by = step)
  train_accuracy <- rep(0, length(nrows))
  cv_accuracy <- rep(0, length(nrows))
  
  n = names(train)
  f = as.formula(paste("survived ~ ", paste(n[!n %in% "survived"], collapse = " + ")))
    
  for (i in 1:length(nrows)) {
    print(nrows[i])
    nn <-  caret::train(f,
                    data = train %>% head(nrows[i]),
                    method = "mlpML",
                    layer1 = l1,
                    layer2 = l2, 
                    layer3 = l3,
                    trControl = nn_control)

    train_accuracy[i] <- nn_accuracy(nn, train %>% head(nrows[i]))$accuracy
    cv_accuracy[i] <- nn_accuracy(nn, test)$accuracy
  }

  data.frame(cbind(nrows, train_accuracy, cv_accuracy))
}

#multilayer_learning_curves(nn_train %>% head(10), nn_test, 10, 5, 0)

```
```{r}
ml_10_5_lcurves <- multilayer_learning_curves(nn_train, nn_test, 25, 10, 5, 0)
write.csv(ml_10_5_lcurves, file = 'output/ml_10_5_lcurves.csv')
#ml_10_5_lcurves <- read_csv(file.path('output','ml_10_5_lcurves.csv'))
plot_learning_curves(ml_10_5_lcurves, "Neural Network with 10 - 5 hidden nodes")
```
```{r}
ml_50_15_lcurves <- multilayer_learning_curves(nn_train, nn_test, 25, 50, 15, 0)
write.csv(ml_50_15_lcurves, file = 'output/ml_50_15_lcurves.csv')
#ml_50_15_lcurves <- read_csv(file.path('output','ml_50_15_lcurves.csv'))
plot_learning_curves(ml_50_15_lcurves, "Neural Network with 50 - 15 hidden nodes")
```
```{r}
ml_100_50_30_lcurves <- multilayer_learning_curves(nn_train, nn_test, 25, 100, 50, 30)
write.csv(ml_100_50_30_lcurves, file = 'output/ml_100_50_30_lcurves.csv')
#ml_100_50_30_lcurves <- read_csv(file.path('output','ml_100_50_30_lcurves.csv'))
plot_learning_curves(ml_100_50_30_lcurves, "Neural Network with 100 - 50 - 30 hidden nodes")
```

```{r}
n = names(nn_train)
f = as.formula(paste("survived ~ ", paste(n[!n %in% "survived"], collapse = " + ")))
nn <-  caret::train(f,
                    data = nn_train,
                    method = "mlpML",
                    layer1 = 100,
                    layer2 = 50, 
                    layer3 = 30,
                    trControl = nn_control)

coef(nn$finalModel)
nn
```
